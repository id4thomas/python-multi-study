# huggingface-transformers

## Parallelize
distribute attention modules of the model across several devices.
```
device_map
```

## References
https://huggingface.co/transformers/model_doc/gpt2.html#gpt2model